{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAptative LINear Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.\n",
    "\n",
    "The difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase, the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights.\n",
    "\n",
    "*A multilayer network of ADALINE units is known as a MADALINE.*\n",
    "\n",
    "<img src=\"files/Img/2.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[0.49999553]\n",
      " [0.4999956 ]] \n",
      "b: [-0.24999474] loss: 0.25 \n",
      "Output:\n",
      " [[-0.24999474]\n",
      " [ 0.25000083]\n",
      " [ 0.25000077]\n",
      " [ 0.74999636]] \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data\n",
    "x_train = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_train = [[0],[0],[0],[1]]\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_uniform((2,1),-1,1))\n",
    "b = tf.Variable(tf.random_uniform((1,),-1,1))\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32,(4,2))\n",
    "y = tf.placeholder(tf.float32,(4,1))\n",
    "Output= tf.matmul(x,W)+b\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(Output - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init) \n",
    "for i in range(800):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss, curr_Output = sess.run([W, b, loss,Output], {x: x_train, y: y_train})\n",
    "print(\"W: %s \\nb: %s loss: %s \\nOutput:\\n %s \"%(curr_W, curr_b, curr_loss,curr_Output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture.\n",
    "\n",
    "\n",
    "<img src=\"files/Img/3.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0022983914 \n",
      "Output\n",
      ": [[1.7269501e-04]\n",
      " [2.4109025e-02]\n",
      " [2.6888039e-02]\n",
      " [9.6846986e-01]] \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data\n",
    "x_train = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_train = [[0],[0],[0],[1]]\n",
    "\n",
    "# Model parameters\n",
    "Wco = tf.Variable(tf.random_uniform((2,10),-1,1))\n",
    "bco = tf.Variable(tf.random_uniform((10,),-1,1))\n",
    "\n",
    "Wcs = tf.Variable(tf.random_uniform((10,1),-1,1))\n",
    "bcs = tf.Variable(tf.random_uniform((1,),-1,1))\n",
    "\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32,(4,2))\n",
    "y = tf.placeholder(tf.float32,(4,1))\n",
    "\n",
    "OutputCo= tf.tanh(tf.matmul(x,Wco)+bco)\n",
    "Output= tf.sigmoid(tf.matmul(OutputCo,Wcs)+bcs)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(Output - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "sess.run(init) \n",
    "for i in range(10000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_Wco, curr_bco, curr_Wcs, curr_bcs,curr_loss, curr_Output = sess.run([Wco, bco,Wcs, bcs, loss,Output], {x: x_train, y: y_train})\n",
    "#print(\"Wco: %s bco: %s Wcs: %s bcs: %s loss: %s Output: %s \"%(curr_Wco, curr_bco, curr_Wcs, curr_bcs,curr_loss,curr_Output))\n",
    "print(\"loss: %s \\nOutput\\n: %s \"%(curr_loss,curr_Output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
