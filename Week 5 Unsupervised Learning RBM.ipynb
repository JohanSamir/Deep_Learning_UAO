{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a branch of machine learning that tries to find hidden structures within unlabeled\n",
    "data and derive insights from it. Clustering, data dimensionality-reduction techniques, noise reduction,\n",
    "segmentation, anomaly detection, fraud detection, and other rich methods rely on unsupervised learning to\n",
    "drive analytics. **Today, with so much data around us, it is impossible to label all data for supervised learning.\n",
    "This makes unsupervised learning all the more important.** Restricted Boltzmann machines and auto-\n",
    "encoders are unsupervised methods that are based on artificial neural networks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzman Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Boltzmann machines are **energy models based on the Boltzmann Distribution Law** of classical\n",
    "physics, where the state of particles of any system is represented by their generalized coordinates and\n",
    "velocities. \n",
    "\n",
    "These generalized coordinates and velocities form the phase space of the particles, and the\n",
    "particles can be in any location in the phase space with specific energy and probability. Let’s consider a\n",
    "classical system that contains N gas molecules and let the generalized position and velocity of any particle\n",
    "be represented by r and v respectively. The location of the particle in the phase space can be\n",
    "represented by (r, v). Every such possible value of (r, v) that the particle can take is called a configuration of\n",
    "the particle. Further, all the N particles are identical in the sense that they are equally likely to take up any\n",
    "state. Given such a system at thermodynamic temperature T, the probability of any such configuration is\n",
    "given as follows:\n",
    "\n",
    "$$\\large P ( r , v ) \\propto e ^ { - \\frac { E ( r , v ) } { K T } }$$\n",
    "\n",
    "E(r, v) is the energy of any particle at configuration (r, v), and K is the Boltzmann Constant. Hence,\n",
    "we see that the probability of any configuration in the phase space is proportional to the exponential of\n",
    "the negative of the energy divided by the product of the Boltzmann Constant and the thermodynamic\n",
    "temperature. To convert the relationship into an equality, the probability needs to be normalized by the sum\n",
    "of the probabilities of all the possible configurations. If there are M possible phase-space configurations for\n",
    "the particles, then the probability of any generalized configuration (r, v) can be expressed as:\n",
    "\n",
    "\n",
    "$$P ( r , v ) = \\frac { e ^ { -  \\large\\frac { E ( r , v ) } { K T } } } { Z }$$\n",
    "\n",
    "where Z is the partition function given by:\n",
    "\n",
    "$$Z = \\sum _ { i = 1 } ^ { M } e ^  { - \\large\\frac { E \\left( ( r , v ) _ { i } \\right) } { K T } }$$\n",
    "\n",
    "M denotes all the unique combinations of r and v possible, which have been denoted by (r, v)i in the preceding equation. If r can take up n distinct\n",
    "coordinate values whereas v can take up m distinct velocity values, then the total number of possible\n",
    "configurations M = n*m . In such cases, the partition function can also be expressed as follows:\n",
    "\n",
    "$$Z = \\sum _ { j = 1 } ^ { m } \\sum _ { i = 1 } ^ { n } e ^ { - \\large\\frac { E \\left( r _ { i } , v _ { j } \\right) } { K T } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood** is the probability of the observed data given the parameters that generate the underlying data.\n",
    "Let’s suppose we observe n observations x1, x2,..... xn and assume that the observations are independent and\n",
    "identically normally distributed with mean μ and variance σ2.\n",
    "\n",
    "The likelihood function in this case would be as follows:  \n",
    "\n",
    "$$P (Data/Model parameters) = P \\left( x _ { 1 } , x _ { 2 } , \\ldots . x _ { n } / \\mu , \\sigma ^ { 2 } \\right)$$\n",
    "\n",
    "Since the observations are independent, we can factorize the likelihood as follows:\n",
    "\n",
    "$$P (Data/Model parameters) = \\prod _ { i = 1 } ^ { n } P \\left( x _ { i } / \\mu , \\sigma ^ { 2 } \\right)$$\n",
    "\n",
    "Each of the:\n",
    "\n",
    "$$x _ { i } \\sim \\operatorname { Normal } \\left( \\mu , \\sigma ^ { 2 } \\right)$$\n",
    "\n",
    "hence the likelihood can be further expanded as follows:\n",
    "\n",
    "$$P(Data/Modelparameters) = \\prod _ { i = 1 } ^ { n } \\frac { 1 } { \\sqrt { 2 \\pi } \\sigma } e ^ { \\large\\frac { - \\left( x _ { i } - \\mu \\right) ^ { 2 } } { 2 \\sigma ^ { 2 } } }.$$\n",
    "\n",
    "Whenever I get data I build a model by defining a likelihood function over the\n",
    "data conditioned on the model parameters and then try to maximize that likelihood function. Likelihood is\n",
    "nothing but the probability of the seen or observed data given the model parameters:\n",
    "\n",
    "$$ Likelihood = P (Data/Model)$$\n",
    "    \n",
    "To get the model defined by its parameters we maximize the likelihood of the seen data:\n",
    "\n",
    "$$Model = Argmax \\hspace{0.4cm} P (Data/Model)$$\n",
    "\n",
    "    \n",
    "**Since we are only trying to fit a model based on the observed data, there is a high chance of overfitting\n",
    "and not generalizing to new data if we go for simple likelihood maximization.**\n",
    "If the data size is huge the seen data is likely to represent the population well and so maximizing the\n",
    "likelihood may suffice. On the other hand, if the seen data is small, there is a high chance that it might not\n",
    "represent the overall population well, and thus the model based on likelihood would not generalize well\n",
    "to new data. In that case, having a certain prior belief over the model and constraining the likelihood by\n",
    "that prior belief would lead to better results. Let’s say that **the prior belief is in the form of our knowing the\n",
    "uncertainty over the model parameters in the form of a probability distribution; i.e., P(Model) is known.** We\n",
    "can in that case update our likelihood by the prior information to get a distribution over the model given the\n",
    "data. As per Bayes’ Theorem of Conditional Probability\n",
    "\n",
    "$$P(\\text {Model} / \\text { Data } ) = \\frac { P ( \\text { Data/Model } ) P ( \\text {Model} ) } { P ( \\text { Data } )}$$\n",
    "\n",
    "\n",
    "**P(Model/Data) is called the posterior distribution** and is generally more informative since it combines\n",
    "one’s prior knowledge about the data or model. Since this probability of data is independent of the model,\n",
    "the posterior is directly proportional to the product of the likelihood and the prior:  \n",
    " \n",
    "\n",
    "$$P (\\text {Model} / \\text { Data } ) \\propto P ( Data/Model) P ( Model)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.ceil vs tf.floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- (Ceil) ----------------\n",
      "\n",
      "[[2. 4. 5.]\n",
      " [4. 4. 4.]\n",
      " [3. 3. 3.]]\n",
      "\n",
      "----------- (Floor) ----------------\n",
      "\n",
      "[[1. 3. 4.]\n",
      " [4. 4. 4.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "t1 = tf.constant([[1.5, 3.5, 4.5], [4, 4, 4], [3, 3, 3]])\n",
    " \n",
    "ceil_tensor = tf.ceil(t1)\n",
    "floor_tensor = tf.floor(t1)\n",
    "exp_tensor = tf.exp(t1)\n",
    "pow_tensor = tf.pow(t1, 2)\n",
    "log_tensor = tf.log(t1)\n",
    " \n",
    "sess = tf.Session()\n",
    " \n",
    "print(\"\\n----------- (Ceil) ----------------\\n\")\n",
    "print(sess.run(ceil_tensor))\n",
    " \n",
    "print(\"\\n----------- (Floor) ----------------\\n\")\n",
    "print(sess.run(floor_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines\n",
    "\n",
    "Restricted Boltzmann machines (RBMs) belong to the unsupervised class of machine-learning algorithms\n",
    "that utilize the Boltzmann Equation of Probability Distribution. It is a two-layer\n",
    "restricted Boltzmann machine architecture that has a hidden layer and a visible layer. There are weight\n",
    "connections between all the hidden and visible layers’ units. However, there are no hidden-to-hidden or\n",
    "visible-to-visible unit connections.\n",
    "\n",
    "The term restricted in RBM refers to this constraint on the network.\n",
    "The hidden units of an RBM are conditionally independent from one another given the set of visible units.\n",
    "\n",
    "In terms of probabilistic graphic models, restricted Boltzmann\n",
    "machines can be defined as undirected probabilistic graphic models containing a visible layer and a single\n",
    "hidden layer. \n",
    "\n",
    "The energy of the joint probability distribution’s having hidden state h and visible state v is given by\n",
    "\n",
    "$$P ( v , h ) = \\frac { e ^ {\\large- E ( h , v ) } } { Z }$$\n",
    "\n",
    "where E(v, h) is the energy of the joint configuration (v, h) and Z is the normalizing factor, commonly\n",
    "known as the partition function. This probability is based on the Boltzmann distribution and assumes the\n",
    "Boltzmann Constant and thermal temperature as 1.\n",
    "\n",
    "$$Z = \\sum _ { v } \\sum _ { h } e ^ { -\\large E ( v , h ) }$$\n",
    "\n",
    "<font color='green'>**RBM are probabilistic. As opposed to assigning discrete values the model assigns probabilities. At each point in time the RBM is in a certain state. The state refers to the values of neurons in the visible and hidden layers v and h. The probability that a certain state of v and h can be observed is given by the joint distribution above.** </font> \n",
    "\n",
    "\n",
    "\n",
    "The energy E(v, h) of the joint configuration (v, h) is given by\n",
    "\n",
    "$$E ( v , h ) = - \\sum _ { i = 1 } ^ { m } b _ { i } v _ { i } - \\sum _ { j = 1 } ^ { n } c _ { j } h _ { j } - \\sum _ { j = 1 } ^ { n } \\sum _ { i = 1 } ^ { m } v _ { i } w _ { i j } h _ { j }$$\n",
    "\n",
    "\n",
    "As it can be noticed **the value of the energy function depends on the configurations of visible/input states, hidden states, weights and biases. The training of RBM consists in finding of parameters for given input values so that the energy reaches a minimum**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## Read the MNIST files\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "## Set up the parameters for training\n",
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "display_step = 1\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "lr = tf.constant(0.001, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Tensor(\"x_1:0\", shape=(?, 784), dtype=float32) \n",
      "W: <tf.Variable 'W_1:0' shape=(784, 500) dtype=float32_ref> \n",
      "b_h: <tf.Variable 'Variable_2:0' shape=(1, 500) dtype=float32_ref> \n",
      "b_v <tf.Variable 'Variable_3:0' shape=(1, 784) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "## Define the tensorflow variables for weights and biases as well as placeholder for input\n",
    "x = tf.placeholder(tf.float32, [None, n_visible], name=\"x\")\n",
    "W = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\")\n",
    "b_h = tf.Variable(tf.zeros([1, n_hidden], tf.float32, name=\"b_h\"))\n",
    "b_v = tf.Variable(tf.zeros([1, n_visible], tf.float32, name=\"b_v\"))\n",
    "print('x:',x,'\\nW:',W,'\\nb_h:',b_h,'\\nb_v',b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts the probability into discrete binary states; i.e., 0 and 1\n",
    "def sample(probs):\n",
    "    #tf.random_uniform(shape,minval=0, maxval=None,dtype=tf.float32,seed=None,name=None)\n",
    "    #tf.floor() \n",
    "    a = tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "    return a\n",
    "\n",
    "## Gibbs sampling step\n",
    "def gibbs_step(x_k):\n",
    "    h_k = sample(tf.sigmoid(tf.matmul(x_k, W) + b_h))\n",
    "    x_k = sample(tf.sigmoid(tf.matmul(h_k, tf.transpose(W)) + b_v))\n",
    "    return x_k\n",
    "\n",
    "## Run multiple Gibbs sampling steps starting from an initial point\n",
    "def gibbs_sample(k,x_k):\n",
    "    for i in range(k):\n",
    "        x_out = gibbs_step(x_k)   \n",
    "       # Returns the Gibbs sample after k iterations\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_s Tensor(\"Floor_43:0\", shape=(?, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Constrastive Divergence algorithm\n",
    "# 1. Through Gibbs sampling locate a new visible state x_sample based on the current visible state x\n",
    "# 2. Based on the new x sample a new h as h_sample\n",
    "x_s = gibbs_sample(2,x)\n",
    "print ('x_s',x_s)\n",
    "h_s = sample(tf.sigmoid(tf.matmul(x_s, W) + b_h))\n",
    "# Sample hidden states based given visible states\n",
    "h = sample(tf.sigmoid(tf.matmul(x, W) + b_h))\n",
    "# Sample visible states based given hidden states\n",
    "x_ = sample(tf.sigmoid(tf.matmul(h, tf.transpose(W)) + b_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Physics we assign a probability to observe a state of v and h, that depends on the overall energy of the model. Unfortunately it is very difficult to calculate the joint probability due to the huge number of possible combination of v and h in the partition function Z. Much easier is the calculation of the conditional probabilities of state h given the state v and conditional probabilities of state v given the state h:\n",
    "\n",
    "$$p ( \\mathbf { h } | \\mathbf { v } ) = \\prod _ { i } p \\left( h _ { i } | \\mathbf { v } \\right)$$\n",
    "$$p ( \\mathbf { v } | \\mathbf { h } ) = \\prod _ { i } p \\left( v _ { i } | \\mathbf { h } \\right)$$\n",
    "\n",
    "\n",
    "It should be noticed beforehand (before demonstrating this fact on practical example) that each neuron in a RBM can only exist in a binary state of 0 or 1. The most interesting factor is the probability that a hidden or visible layer neuron is in the state 1 — hence activated. Given an input vector v the probability for a single hidden neuron j being activated is:\n",
    "$$p \\left( h _ { j } = 1 | \\mathbf { v } \\right) = \\frac { 1 } { 1 + e ^ { \\left( - \\left( b _ { j } + W _ { j } v _ { i } \\right) \\right) } } = \\sigma \\left( b _ { j } + \\sum _ { i } v _ { i } w _ { i j } \\right)$$\n",
    "\n",
    "$$p \\left( v _ { i } = 1 | \\mathbf { h } \\right) = \\frac { 1 } { 1 + e ^ { \\left( - \\left( a _ { i } + W _ { i } h _ { j } \\right) \\right) } } = \\sigma \\left( a _ { i } + \\sum _ { j } h _ { j } w _ { i j } \\right)$$\n",
    "\n",
    "Here is σ the Sigmoid function.\n",
    "\n",
    "\n",
    "**Computing the Gradients**\n",
    "\n",
    "The values obtained in the previous step can be used to compute the gradient matrix and the gradient vectors\n",
    "The update of the weight matrix happens during the Contrastive Divergence step.   \n",
    "\n",
    "$$\\triangle W = \\mathbf { v } _ { 0 } \\otimes p \\left( \\mathbf { h } _ { 0 } | \\mathbf { v } _ { 0 } \\right) - \\mathbf { v } _ { k } \\otimes p \\left( \\mathbf { h } _ { k } | \\mathbf { v } _ { k } \\right)$$\n",
    "\n",
    "$$\\triangle b = p \\left( \\mathbf { h } _ { 0 } | \\mathbf { v } _ { 0 } \\right) - p \\left( \\mathbf { h } _ { k } | \\mathbf { v } _ { k } \\right)$$\n",
    "\n",
    "$$\\Delta a = \\mathbf { v } _ { 0 } - \\mathbf { v } _ { k }$$\n",
    "\n",
    "Using the update matrix the new weights can be calculated with gradient ascent, given by:\n",
    "\n",
    "$$W _ { n e w } = W _ { o l d } + \\triangle W$$  \n",
    "$$a _ { n e w } = a _ { o l d } + \\Delta a$$  \n",
    "$$b _ { n e w } = b _ { o l d } + \\triangle b$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight updated based on gradient descent\n",
    "size_batch = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "W_add = tf.multiply(lr/size_batch, tf.subtract(tf.matmul(tf.transpose(x), h), tf.matmul(tf.transpose(x_s), h_s)))\n",
    "\n",
    "bv_add = tf.multiply(lr/size_batch, tf.reduce_sum(tf.subtract(x, x_s), 0, True))\n",
    "bh_add = tf.multiply(lr/size_batch, tf.reduce_sum(tf.subtract(h, h_s), 0, True))\n",
    "\n",
    "updt = [W.assign_add(W_add), b_v.assign_add(bv_add), b_h.assign_add(bh_add)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Restricted Boltzmann Machine\n",
    "\n",
    "We need to train the Boltzmann machine in order to derive the model parameters b, c, W, where b and c are\n",
    "the bias vectors at the visible and hidden units respectively and W is the weight-connections matrix between\n",
    "the visible and hidden layers.  \n",
    "\n",
    "For ease of reference, the model parameters can be collectively referred to as:\n",
    "\n",
    "$$\\theta = [ b ; c ; W ]$$\n",
    "\n",
    "\n",
    "**Gibbs sampler**\n",
    "\n",
    "Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution.\n",
    "\n",
    "Suppose p(x, y) is a p.d.f. or p.m.f. that is difficult to sample from directly.\n",
    "\n",
    "1. Suppose, though, that we can easily sample from the conditional distributions p(x|y) and p(y|x).\n",
    "2. The Gibbs sampler proceeds as follows:\n",
    "    1. set x and y to some initial starting values\n",
    "    2. then sample x|y, then sample y|x, then x|y, and so on.\n",
    "\n",
    "<img src=\"files/Img/4.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This procedure defines a sequence of pairs of random variables\n",
    "(X0, Y0),(X1, Y1),(X2, Y2),(X3, Y3), . . .\n",
    "\n",
    "(X0, Y0),(X1, Y1),(X2, Y2),(X3, Y3), . . .\n",
    "satisfies the property of being a Markov chain.\n",
    "The conditional distribution of (Xi\n",
    ", Yi) given all of the previous\n",
    "pairs depends only on (Xi−1, Yi−1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow graph execution\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables of the Model\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "  \n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Start the training\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run the weight update\n",
    "            batch_xs = (batch_xs > 0)*1\n",
    "            _ = sess.run([updt], feed_dict={x:batch_xs})\n",
    "        # Display the running step\n",
    "        #if epoch % display_step == 0:\n",
    "            #print(\"Epoch:\", '%04d' % (epoch+1))\n",
    "     \n",
    "    print(\"RBM training Completed !\")\n",
    "    ## Generate hidden structure for 1st 20 images in test MNIST\n",
    "    out = sess.run(h,feed_dict={x:(mnist.test.images[:20]> 0)*1})\n",
    "    label = mnist.test.labels[:20]\n",
    "    ## Take the hidden representation of any of the test images; i.e., the 3rd record\n",
    "    ## The output level of the 3rd record should match the image generated\n",
    "    plt.figure(1)\n",
    "    for k in range(20):\n",
    "        plt.subplot(4, 5, k+1)\n",
    "        image = (mnist.test.images[k]> 0)*1\n",
    "        image = np.reshape(image,(28,28))\n",
    "        plt.imshow(image,cmap='gray')\n",
    "\n",
    "    plt.figure(2)\n",
    "    for k in range(20):\n",
    "        plt.subplot(4, 5, k+1)\n",
    "        image = sess.run(x_,feed_dict={h:np.reshape(out[k],(-1,n_hidden))})\n",
    "        image = np.reshape(image,(28,28))\n",
    "        plt.imshow(image,cmap='gray')\n",
    "        print(np.argmax(label[k]))\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deep belief networks (DBNs)\n",
    "\n",
    "Deep belief networks are based on the restricted Boltzmann machine but, unlike an RBN, a DBN has\n",
    "multiple hidden layers. The weights in each hidden layer K are trained by keeping all the weights in the prior\n",
    "( K - 1 ) layers constant. The activities of the hidden units in the ( K -1) layer are taken as input for the\n",
    "Kth layer. At any particular time during training two layers are involved in learning the weight connections\n",
    "between them. The learning algorithm is the same as that of restricted Boltzmann machines\n",
    "\n",
    "The DBN\n",
    "learning algorithm is used to learn the initial weights of a deep network being used for supervised learning\n",
    "so that the network has a good set of initial weights to start with. Once the pre-training is done for the deep\n",
    "belief network, we can add an output layer to the DBN based on the supervised problem at hand\n",
    "\n",
    "<font color='green'> Let’s say we want to train a model to perform classification on the MNIST dataset. In that case, we would have to\n",
    "append a ten-class SoftMax layer. We can then fine-tune the model by using backpropagation of error. Since\n",
    "the model already has an initial set of weights from unsupervised DBN learning, the model would have a\n",
    "good chance of converging faster when backpropagation is invoked.<font>\n",
    "\n",
    "We now look at an implementation of DBN pre-training of weights followed by the training of a\n",
    "classification network by appending the output layer to the hidden layer of the RBM. We\n",
    "implemented RBM, wherein we learned the weights of the visible-to-hidden connections, assuming all the\n",
    "units are sigmoid. To that RBM we are going to stack the output layer of ten classes for the MNIST dataset\n",
    "and train the classification model using the weights from the visible-to-hidden units learned as the initial\n",
    "weights for the classification network. Of course, we would have a new set of weights corresponding to the\n",
    "connection of the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "## Read the MNIST files\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "## Set up the parameters for training\n",
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "display_step = 1\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "lr = tf.constant(0.001, tf.float32)\n",
    "learning_rate_train = tf.constant(0.01, tf.float32)\n",
    "n_classes = 10\n",
    "training_iters = 200\n",
    "## Define the tensorflow variables for weights and biases as well as placeholder forx = tf.placeholder(tf.float32, [None, n_visible], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None,10], name=\"y\")\n",
    "W = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\")\n",
    "b_h = tf.Variable(tf.zeros([1, n_hidden], tf.float32, name=\"b_h\"))\n",
    "b_v = tf.Variable(tf.zeros([1, n_visible], tf.float32, name=\"b_v\"))\n",
    "W_f = tf.Variable(tf.random_normal([n_hidden,n_classes], 0.01), name=\"W_f\")\n",
    "b_f = tf.Variable(tf.zeros([1, n_classes], tf.float32, name=\"b_f\"))\n",
    "## Converts the probability into discrete binary states i.e. 0 and 1\n",
    "def sample(probs):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "1. http://deeplearning.net/tutorial/rbm.html\n",
    "2. http://web.science.mq.edu.au/~mjohnson/papers/Johnson12IntroRBMtalk.pdf\n",
    "3. https://arxiv.org/pdf/1806.07066.pdf\n",
    "4. https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "5. https://www.slideshare.net/sogo1127/learning-rbmrestricted-boltzmann-machine-in-practice\n",
    "6. https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n",
    "7. http://www2.stat.duke.edu/~rcs46/modern_bayes17/lecturesModernBayes17/lecture-7/07-gibbs.pdf\n",
    "8. http://www.uta.fi/sis/tie/neuro/index/Neurocomputing7.pdf\n",
    "9. https://www.cse.unsw.edu.au/~cs9444/17s2/lect/1page/11_Boltzmann.pdf\n",
    "10. http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
